{
  "hash": "f5f0264d7cea0e005ea1b3cae0a72e84",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: styles.css\n    slide-number: \"c/t\"\n    overview: true\n    progress: true\n    mouse-wheel: false\n    controls: true\n    incremental: true\n    lang: \"en\"\n    pagetitle: \"DynAMoS Database Slides\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2023-09-13\"\n---\n\n\n\n::: {.my-title}\n# DynAMoS Database\n[Dynamic Affective Movie Clip<br />Database for Subjectivity Analysis]{.my-subtitle}\n\n::: {.my-details}\n[ACII 2023 | Girard, Tie, & Liebenthal]{}<br />\n{{< fa link size=xs >}} [<https://dynamos.mgb.org>]{.p90}\n:::\n\n![](movie.svg){.absolute bottom=20 right=0 width=360}\n:::\n\n## Types of Emotion Ratings {.p90}\n\n- **Who is providing the measure of emotion?**\n    + [Self]{.b .blue}-reports of participants' own emotion\n    + [Observer]{.b .blue}-perceptions of others' emotion\n\n- **How is emotion being represented?**\n    + [Discrete]{.b .blue} choices of emotion categories (e.g, happy, angry)\n    + [Continuous]{.b .blue} scores on emotion dimensions (e.g., valence)\n    \n- **How often are the emotions reported?**\n    + [Holistic]{.b .blue} ratings collected once after each stimulus\n    + [Dynamic]{.b .blue} ratings collected repeatedly during each stimulus\n\n## Ambiguity and Subjectivity\n\n- Emotion ratings will inevitably vary between raters...\n\n- We usually treat such variability as a ***nuisance*** to \"fix\"\n    \n- But we can gain a lot by embracing these differences: studying their sources and building them into our models\n    \n- [Ambiguity]{.b .blue}: variability across different *observers' perceptions* of the emotion in a given stimulus (see [Sethu et al., 2019](https://arxiv.org/abs/1909.00360))\n\n- [Subjectivity]{.b .red}: variability across different *subjects' self-reports* of the emotion they experienced from a given stimulus\n\n## DynAMoS Database\n\n::: {.nonincremental}\n- **Participants**\n    + Healthy community members from *Rally with MGB*\n    + 83 participants (67% female, 18--59 years old)\n    + 43 White, 22 Asian, 12 Black, 5 Other; 11 Hispanic/Latino\n\n- **Procedure**\n    + Watch 22 affective movie clips (2.2--7.1 minutes each)\n    + Dynamic self-reported emotional valence ([CARMA](https://carma.jmgirard.com))\n    + Holistic self-reported positive/negative affect ([S-PANAS](10.1016/S0191-8869(98)00251-7))\n:::\n\n## Emotional Ratings\n\n::: {.columns}\n::: {.column}\n### Dynamic\n--4 (*negative*) to +4 (*positive*)<br>\nRated at 30 Hz, binned to 1 Hz\n![](fig_carma.png)\n:::\n::: {.column}\n### Holistic \nEach rated 0 (*very slightly or not at all*) to 4 (*extremely*):\n\n[Positive Affect]{.blue}<br>alert, determined, enthusiastic, excited, inspired\n\n[Negative Affect]{.blue}<br>afraid, distressed, nervous, upset, scared\n\n:::\n:::\n\n::: {.footer}\n<https://carma.jmgirard.com>\n:::\n\n## Quantifying Subjectivity\n\n![](river.png)\n\n::: {.footer}\nEstimates from a Bayesian G study ([github.com/jmgirard/varde](https://github.com/jmgirard/varde))\n:::\n\n## Visualizing many time series is hard...\n\n\n\n::: {.cell hash='Presentation_cache/html/unnamed-chunk-1_79265c7f47fe9b269a11115c638172cb'}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 239 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](Presentation_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n::: {.footer}\nEach blue line is one participant's time series\n:::\n\n## Introducing the Chromodoris plot\n\n\n\n::: {.cell hash='Presentation_cache/html/unnamed-chunk-2_8c8a65aaabed25708fd192d13dadfc97'}\n::: {.cell-output-display}\n![](Presentation_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n::: {.footer}\nBlack = Mean, Yellow = 50%, Green = 70%, Purple = 90%\n:::\n\n## Database Uses\n\n- **Emotion elicitation** video set with normative data \n\n- Affective **content analysis** with average ratings\n\n- Subjectivity analysis to predict **rating distributions**\n\n- Subjectivity analysis to explain **degree of subjectivity**\n\n- **Personalized modeling** of affective reactions\n\n## Future Directions\n\n1. Add more movie **clips** and **participants**\n\n1. Add more dynamic and holistic rating **dimensions**\n\n1. Add data from **sensors** (e.g., physiological and eye tracking)\n\n1. Collect information about participants' **personality**\n\n1. Collect similar data in **clinical/medical** populations\n\n## Acknowledgements\n\n::: {.nonincremental}\n\n- **Co-authors**\n    + Jeffrey Girard (University of Kansas)\n    + Yanmei Tie (Brigham & Women's Hospital, HMS)\n    + Einat Liebenthal (McLean Hospital, HMS)\n\n- **Funding**\n    + Alexandra Golby (Brigham & Women's Hospital, HMS)\n\n- **Assistance**\n    + Colin Gavin, Laura Rigolo, Abby Recko, Ben Phan\n    \n:::\n    \n## Questions? {.tc}\n\n![](movie.svg)\n\n<https://dynamos.mgb.org>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}